# Reddit users banned for "interference" 2018 - 2019
### Context:
In 2018 Reddit banned 944 accounts on suspicion of interference by the Russian government:
[First "banwave" where 944 users were banned from reddit in 2018](https://www.reddit.com/r/announcements/comments/8bb85p/reddits_2017_transparency_report_and_suspect/)

A year later, it again banned 61 accounts however this time the target of the suspicious accounts was the UK Election of December 2019:
[Second wave of banning users just before the UK elections in December 2019](https://www.reddit.com/r/announcements/duplicates/e75f07/suspected_campaign_from_russia_on_reddit/)

Data comes from a JSON file generated by a python script (reddit-get-data.py) that scrapes the users profile listed in these two links.

### Data preparation


```python
import pandas as pd
from pandas import json_normalize
import json
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud, STOPWORDS 

from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
%matplotlib inline

```


```python
# sentiment analysis, wordcloud

%pip install wordcloud
import nltk
nltk.download('vader_lexicon')
nltk.download("popular")
nltk.download('punkt')
```

    Requirement already satisfied: wordcloud in /Users/k/opt/anaconda3/lib/python3.7/site-packages (1.6.0)
    Requirement already satisfied: numpy>=1.6.1 in /Users/k/opt/anaconda3/lib/python3.7/site-packages (from wordcloud) (1.17.2)
    Requirement already satisfied: matplotlib in /Users/k/opt/anaconda3/lib/python3.7/site-packages (from wordcloud) (3.1.1)
    Requirement already satisfied: pillow in /Users/k/opt/anaconda3/lib/python3.7/site-packages (from wordcloud) (6.2.0)
    Requirement already satisfied: cycler>=0.10 in /Users/k/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (0.10.0)
    Requirement already satisfied: kiwisolver>=1.0.1 in /Users/k/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.1.0)
    Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/k/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.4.2)
    Requirement already satisfied: python-dateutil>=2.1 in /Users/k/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.8.0)
    Requirement already satisfied: six in /Users/k/opt/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.12.0)
    Requirement already satisfied: setuptools in /Users/k/opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.4.0)
    Note: you may need to restart the kernel to use updated packages.


    [nltk_data] Downloading package vader_lexicon to /Users/k/nltk_data...
    [nltk_data]   Package vader_lexicon is already up-to-date!
    [nltk_data] Downloading collection 'popular'
    [nltk_data]    | 
    [nltk_data]    | Downloading package cmudict to /Users/k/nltk_data...
    [nltk_data]    |   Package cmudict is already up-to-date!
    [nltk_data]    | Downloading package gazetteers to
    [nltk_data]    |     /Users/k/nltk_data...
    [nltk_data]    |   Package gazetteers is already up-to-date!
    [nltk_data]    | Downloading package genesis to /Users/k/nltk_data...
    [nltk_data]    |   Package genesis is already up-to-date!
    [nltk_data]    | Downloading package gutenberg to
    [nltk_data]    |     /Users/k/nltk_data...
    [nltk_data]    |   Package gutenberg is already up-to-date!
    [nltk_data]    | Downloading package inaugural to
    [nltk_data]    |     /Users/k/nltk_data...
    [nltk_data]    |   Package inaugural is already up-to-date!
    [nltk_data]    | Downloading package movie_reviews to
    [nltk_data]    |     /Users/k/nltk_data...
    [nltk_data]    |   Package movie_reviews is already up-to-date!
    [nltk_data]    | Downloading package names to /Users/k/nltk_data...
    [nltk_data]    |   Package names is already up-to-date!
    [nltk_data]    | Downloading package shakespeare to
    [nltk_data]    |     /Users/k/nltk_data...
    [nltk_data]    |   Package shakespeare is already up-to-date!
    [nltk_data]    | Downloading package stopwords to
    [nltk_data]    |     /Users/k/nltk_data...
    [nltk_data]    |   Package stopwords is already up-to-date!
    [nltk_data]    | Downloading package treebank to /Users/k/nltk_data...
    [nltk_data]    |   Package treebank is already up-to-date!
    [nltk_data]    | Downloading package twitter_samples to
    [nltk_data]    |     /Users/k/nltk_data...
    [nltk_data]    |   Package twitter_samples is already up-to-date!
    [nltk_data]    | Downloading package omw to /Users/k/nltk_data...
    [nltk_data]    |   Package omw is already up-to-date!
    [nltk_data]    | Downloading package wordnet to /Users/k/nltk_data...
    [nltk_data]    |   Package wordnet is already up-to-date!
    [nltk_data]    | Downloading package wordnet_ic to
    [nltk_data]    |     /Users/k/nltk_data...
    [nltk_data]    |   Package wordnet_ic is already up-to-date!
    [nltk_data]    | Downloading package words to /Users/k/nltk_data...
    [nltk_data]    |   Package words is already up-to-date!
    [nltk_data]    | Downloading package maxent_ne_chunker to
    [nltk_data]    |     /Users/k/nltk_data...
    [nltk_data]    |   Package maxent_ne_chunker is already up-to-date!
    [nltk_data]    | Downloading package punkt to /Users/k/nltk_data...
    [nltk_data]    |   Package punkt is already up-to-date!
    [nltk_data]    | Downloading package snowball_data to
    [nltk_data]    |     /Users/k/nltk_data...
    [nltk_data]    |   Package snowball_data is already up-to-date!
    [nltk_data]    | Downloading package averaged_perceptron_tagger to
    [nltk_data]    |     /Users/k/nltk_data...
    [nltk_data]    |   Package averaged_perceptron_tagger is already up-
    [nltk_data]    |       to-date!
    [nltk_data]    | 
    [nltk_data]  Done downloading collection popular
    [nltk_data] Downloading package punkt to /Users/k/nltk_data...
    [nltk_data]   Package punkt is already up-to-date!





    True




```python
# import the json file containint data on 1001 banned Reddit users
rawdata = pd.read_json("data-files/reddit-banned-users-DATA.json")
```


```python
# reverse rows/columns T short for transverse
rawdata = rawdata.T
```


```python
rawdata.shape
```




    (1000, 6)




```python
#normalizing "about" column, including suspended accounts
about = json_normalize(rawdata["about"])
```


```python
# appending banwave_year to about dataframe
about["banwave_year"] = rawdata["banwave_year"].values
```


```python
rawdata.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>about</th>
      <th>submitted</th>
      <th>comments</th>
      <th>gilded</th>
      <th>trophies</th>
      <th>banwave_year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1488Reasons</th>
      <td>{'is_employee': False, 'icon_img': 'https://ww...</td>
      <td>{'modhash': '', 'dist': 1, 'children': [{'kind...</td>
      <td>{'modhash': '', 'dist': 25, 'children': [{'kin...</td>
      <td>{'modhash': '', 'dist': 0, 'children': [], 'af...</td>
      <td>{'trophies': [{'kind': 't6', 'data': {'icon_70...</td>
      <td>2018-May-09</td>
    </tr>
    <tr>
      <th>20twony</th>
      <td>{'is_employee': False, 'icon_img': 'https://ww...</td>
      <td>{'modhash': '', 'dist': 1, 'children': [{'kind...</td>
      <td>{'modhash': '', 'dist': 0, 'children': [], 'af...</td>
      <td>{'modhash': '', 'dist': 0, 'children': [], 'af...</td>
      <td>{'trophies': [{'kind': 't6', 'data': {'icon_70...</td>
      <td>2018-May-09</td>
    </tr>
    <tr>
      <th>Abena_Tau</th>
      <td>{'is_employee': False, 'icon_img': 'https://ww...</td>
      <td>{'modhash': '', 'dist': 18, 'children': [{'kin...</td>
      <td>{'modhash': '', 'dist': 0, 'children': [], 'af...</td>
      <td>{'modhash': '', 'dist': 0, 'children': [], 'af...</td>
      <td>{'trophies': [{'kind': 't6', 'data': {'icon_70...</td>
      <td>2018-May-09</td>
    </tr>
    <tr>
      <th>Admiraf</th>
      <td>{'is_employee': False, 'icon_img': 'https://ww...</td>
      <td>{'modhash': '', 'dist': 1, 'children': [{'kind...</td>
      <td>{'modhash': '', 'dist': 0, 'children': [], 'af...</td>
      <td>{'modhash': '', 'dist': 0, 'children': [], 'af...</td>
      <td>{'trophies': [{'kind': 't6', 'data': {'icon_70...</td>
      <td>2018-May-09</td>
    </tr>
    <tr>
      <th>AdofynMorakus</th>
      <td>{'is_employee': False, 'icon_img': 'https://ww...</td>
      <td>{'modhash': '', 'dist': 1, 'children': [{'kind...</td>
      <td>{'modhash': '', 'dist': 0, 'children': [], 'af...</td>
      <td>{'modhash': '', 'dist': 0, 'children': [], 'af...</td>
      <td>{'trophies': [{'kind': 't6', 'data': {'icon_70...</td>
      <td>2018-May-09</td>
    </tr>
  </tbody>
</table>
</div>




```python
# isolating suspended account in a separate dataframe
suspended = rawdata.loc[[x for x in rawdata.index if "is_suspended" in rawdata.loc[x]["about"]]]
```


```python
#removing it to clean the rest of the data
rawdata.drop(suspended.index, axis=0, inplace=True)
```


```python
#deeper levels of nesting
temp_trophies = json_normalize(rawdata["trophies"])
gilded = json_normalize(rawdata["gilded"])

# submitted and comments have deeper levels of nested json
temp_submitted = json_normalize(rawdata["submitted"])
temp_comments = json_normalize(rawdata["comments"])
```


```python
# get to the "data" field in nested levels for "comments", "submitted" and "trophies"
def go_to_data(node):
    lst = []
    for i in range(len(node)): # iterate through data series
        entry = node.loc[i] # for clarity
        if "children" in node:
            container = "children"
        elif "trophies" in node:
            container = "trophies"
        if len(entry[container]) > 0: 
            for j in range(len(entry[container])):
                lst.append(entry[container][j]["data"])
    return json_normalize(lst)
```


```python
submitted =  go_to_data(temp_submitted)
comments =  go_to_data(temp_comments)
trophies = go_to_data(temp_trophies)
```


```python
submitted.shape
```




    (3933, 359)




```python
submitted.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>approved_at_utc</th>
      <th>subreddit</th>
      <th>selftext</th>
      <th>author_fullname</th>
      <th>saved</th>
      <th>mod_reason_title</th>
      <th>gilded</th>
      <th>clicked</th>
      <th>title</th>
      <th>link_flair_richtext</th>
      <th>...</th>
      <th>media_metadata.o6t2xme0b9u31.s.u</th>
      <th>media_metadata.o6t2xme0b9u31.m</th>
      <th>media_metadata.o6t2xme0b9u31.id</th>
      <th>media_metadata.lhjpjj6furt21.status</th>
      <th>media_metadata.lhjpjj6furt21.e</th>
      <th>media_metadata.lhjpjj6furt21.s.y</th>
      <th>media_metadata.lhjpjj6furt21.s.x</th>
      <th>media_metadata.lhjpjj6furt21.s.u</th>
      <th>media_metadata.lhjpjj6furt21.m</th>
      <th>media_metadata.lhjpjj6furt21.id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>None</td>
      <td>u_reddit</td>
      <td></td>
      <td>t2_1qwk</td>
      <td>False</td>
      <td>None</td>
      <td>0</td>
      <td>False</td>
      <td>This account is banned and is temporarily pres...</td>
      <td>[]</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>None</td>
      <td>u_reddit</td>
      <td></td>
      <td>t2_1qwk</td>
      <td>False</td>
      <td>None</td>
      <td>0</td>
      <td>False</td>
      <td>This account is banned and is temporarily pres...</td>
      <td>[]</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>None</td>
      <td>u_reddit</td>
      <td></td>
      <td>t2_1qwk</td>
      <td>False</td>
      <td>None</td>
      <td>0</td>
      <td>False</td>
      <td>This account is banned and is temporarily pres...</td>
      <td>[]</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>None</td>
      <td>Blackpeople</td>
      <td></td>
      <td>t2_wr8hk</td>
      <td>False</td>
      <td>None</td>
      <td>0</td>
      <td>False</td>
      <td>The Story of 10 Young People Killed in a Day b...</td>
      <td>[]</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>None</td>
      <td>Blackpeople</td>
      <td></td>
      <td>t2_wr8hk</td>
      <td>False</td>
      <td>None</td>
      <td>0</td>
      <td>False</td>
      <td>Ramsey Orta: No regrets for shooting viral vid...</td>
      <td>[]</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 359 columns</p>
</div>




```python
comments.shape
```




    (1776, 72)




```python
comments.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>awarders</th>
      <th>total_awards_received</th>
      <th>approved_at_utc</th>
      <th>link_title</th>
      <th>mod_reason_by</th>
      <th>banned_by</th>
      <th>author_flair_type</th>
      <th>removal_reason</th>
      <th>link_id</th>
      <th>author_flair_template_id</th>
      <th>...</th>
      <th>controversiality</th>
      <th>locked</th>
      <th>author_flair_background_color</th>
      <th>collapsed_because_crowd_control</th>
      <th>mod_reports</th>
      <th>quarantine</th>
      <th>subreddit_type</th>
      <th>ups</th>
      <th>gildings.gid_1</th>
      <th>gildings.gid_2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[]</td>
      <td>0</td>
      <td>None</td>
      <td>The top tenth of Kiwis hold well over half of ...</td>
      <td>None</td>
      <td>None</td>
      <td>text</td>
      <td>None</td>
      <td>t3_4q6zhk</td>
      <td>None</td>
      <td>...</td>
      <td>0</td>
      <td>False</td>
      <td></td>
      <td>None</td>
      <td>[]</td>
      <td>False</td>
      <td>public</td>
      <td>-6</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[]</td>
      <td>0</td>
      <td>None</td>
      <td>NZ Muslims returning home complain of 'profili...</td>
      <td>None</td>
      <td>None</td>
      <td>text</td>
      <td>None</td>
      <td>t3_4q7hpd</td>
      <td>None</td>
      <td>...</td>
      <td>1</td>
      <td>False</td>
      <td></td>
      <td>None</td>
      <td>[]</td>
      <td>False</td>
      <td>public</td>
      <td>7</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[]</td>
      <td>0</td>
      <td>None</td>
      <td>The top tenth of Kiwis hold well over half of ...</td>
      <td>None</td>
      <td>None</td>
      <td>text</td>
      <td>None</td>
      <td>t3_4q6zhk</td>
      <td>None</td>
      <td>...</td>
      <td>0</td>
      <td>False</td>
      <td></td>
      <td>None</td>
      <td>[]</td>
      <td>False</td>
      <td>public</td>
      <td>-8</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[]</td>
      <td>0</td>
      <td>None</td>
      <td>The top tenth of Kiwis hold well over half of ...</td>
      <td>None</td>
      <td>None</td>
      <td>text</td>
      <td>None</td>
      <td>t3_4q6zhk</td>
      <td>None</td>
      <td>...</td>
      <td>0</td>
      <td>False</td>
      <td></td>
      <td>None</td>
      <td>[]</td>
      <td>False</td>
      <td>public</td>
      <td>-6</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[]</td>
      <td>0</td>
      <td>None</td>
      <td>You Shouldn’t Dream Here: On the tragic Auckla...</td>
      <td>None</td>
      <td>None</td>
      <td>text</td>
      <td>None</td>
      <td>t3_4no6eh</td>
      <td>None</td>
      <td>...</td>
      <td>1</td>
      <td>False</td>
      <td></td>
      <td>None</td>
      <td>[]</td>
      <td>False</td>
      <td>public</td>
      <td>-5</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 72 columns</p>
</div>




```python
# dropping duplicates values
submitted.drop_duplicates(subset="name", keep='first', inplace=True)
comments.drop_duplicates(subset="body", keep='first', inplace=True)
about.drop_duplicates(subset="name", keep='first', inplace=True)
```


```python
# isolating nested media_metadata data: url (s.u) type (m))
metadata_col_type = [ x for x in [x for x in submitted.columns if "media_metadata" in x] if ".m" in x]
metadata_col_url = [ x for x in [x for x in submitted.columns if "media_metadata" in x] if "s.u" in x]
metadata_col_id = [ x for x in [x for x in submitted.columns if "media_metadata" in x] if ".id" in x]
```


```python
# consolidating the 60 metadata id and url columns into two new columns
submitted["metadata_type"] = submitted[metadata_col_type].apply(lambda x: "".join(x.dropna().astype(str)), axis=1)
submitted["metadata_url"] = submitted[metadata_col_url].apply(lambda x: "".join(x.dropna().astype(str)), axis=1)
submitted["metadata_id"] = submitted[metadata_col_id].apply(lambda x: "".join(x.dropna().astype(str)), axis=1)
```


```python
# removing columns with "media_metadata" except type ("m") unique id and url
submitted.drop([x for x in submitted.columns if "media_metadata" in x and x != "metadata_col_type" and x != "metadata_col_url" and x != "metadata_col_id"], axis=1, inplace=True)
submitted.drop([x for x in submitted.columns if "secure_media" in x], axis=1, inplace=True)
submitted.drop([x for x in submitted.columns if "media_embed" in x], axis=1, inplace=True)
# removing media_oembed columns except "provider_name", "type", "url", "author_name"
submitted.drop([x for x in submitted.columns if "media.oembed" in x and x != "media.oembed.provider_name" and x != "media.oembed.type" and x != "media.oembed.url" and x != "media.oembed.author_name"], axis=1, inplace=True)

#removing columns about "flair"
submitted.drop([x for x in submitted.columns if "flair" in x], axis=1, inplace=True)
comments.drop([x for x in comments.columns if "flair" in x], axis=1, inplace=True)
#removing columns about "thumbnail"
submitted.drop([x for x in submitted.columns if "thumbnail" in x], axis=1, inplace=True)
comments.drop([x for x in comments.columns if "thumbnail" in x], axis=1, inplace=True)
# removing columns with "subreddit" in "about" since they all contain users subreddits
about.drop([x for x in about.columns if "subreddit" in x], axis=1, inplace=True)
# removing the u_reddit subreddit
submitted.drop(submitted[submitted["subreddit"] == "u_reddit"].index, inplace=True)
# dropping columns with all n/a values
submitted.dropna(axis=1, how="all", inplace=True)
comments.dropna(axis=1, how="all", inplace=True)
about.dropna(axis=1, how="all", inplace=True)
#other columns
to_drop_submitted = ["subreddit_name_prefixed", "wls", "pwls", "allow_live_comments", 
                     "content_categories", "preview.enabled", "whitelist_status", "send_replies",
                    "post_hint", "is_robot_indexable","suggested_sort", "author_premium", "all_awardings",
                    "preview.images", "preview.enabled", "subreddit_id", "parent_whitelist_status"]
to_drop_comments = ["subreddit_name_prefixed", "all_awardings", "author_premium", "permalink"]
submitted.drop(to_drop_submitted, axis=1, inplace=True)
comments.drop(to_drop_comments, axis=1, inplace=True)
```


```python
# rename gildings gid_1 silver gid_2 gold
submitted = submitted.rename(columns={"gildings.gid_1": "gilded_silver", "gildings.gid_2": "gilded_gold"})
comments = comments.rename(columns={"gildings.gid_1": "gilded_silver", "gildings.gid_2": "gilded_gold"})
# creating new column in about for total karma
about["karma"] = about["comment_karma"] + about["link_karma"]
```


```python
# select all columns where same value is in all rows
def show_same_values(node):
    return [x for x in node.columns if node[x].value_counts().iloc[0] == len(node[x])]
```


```python
# keeping these columns in a separate dataframe just in case
same_values_submitted = submitted[show_same_values(submitted)]
same_values_comments = comments[show_same_values(comments)]
same_values_about = about[show_same_values(about)]
```


```python
# drop these same values columns from the data
submitted.drop(same_values_submitted, axis=1, inplace=True)
comments.drop(same_values_comments, axis=1, inplace=True)
```


```python
# merging all back - just in case
#all_data = about.merge(submitted, how="inner", left_on="name", right_on="author").merge(comments, how="inner", left_on="author", right_on="author")
```


```python
# date conversion of "created_utc" and "edited" fields in "comments" and "submitted" tables
def convert_dates(*args):
    for node in args: #dealing with subsets
        if "created_x_utc" in node.columns:
            node["date_posted"] = pd.to_datetime(node["created_x_utc"], unit="s")
        else:
            node["date_created"] = pd.to_datetime(node["created_utc"], unit="s")
            node["year_graph"] = pd.DatetimeIndex(node["date_created"]).year
            node["year"] = pd.PeriodIndex(node["date_created"], freq='A')
            node["month_nr"] = pd.PeriodIndex(node["date_created"], freq="M")
            node["month"] = pd.DatetimeIndex(node["date_created"]).month_name()
            node["day"] =pd.PeriodIndex(node["date_created"], freq="D")
            node["day_of_week"] = pd.DatetimeIndex(node["date_created"]).day_name()       

convert_dates(submitted, comments, about)
# rename "date_created" columns in submitted and columns to avoid confusion
submitted.rename(columns={"date_created": "date_posted"}, inplace=True)
comments.rename(columns={"date_created": "date_posted"}, inplace=True)
```


```python
# creating subsets reflecting the 2018 and 2019 banwaves
about_banwave_2018 = about[about["banwave_year"] == "2018-May-09"]
about_banwave_2019 = about[about["banwave_year"] == "2019-Dec-06"]

# merging with submitted and comments df using "about["name"]" and "["author"]" as key
submitted_banwave_2018 = submitted.merge(about_banwave_2018, how="inner", left_on="author", right_on="name")
submitted_banwave_2019 = submitted.merge(about_banwave_2019, how="inner", left_on="author", right_on="name")
comments_banwave_2018 = comments.merge(about_banwave_2018, how="inner", left_on="author", right_on="name")
comments_banwave_2019 = comments.merge(about_banwave_2019, how="inner", left_on="author", right_on="name")
```


```python
submitted_banwave_2018.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>subreddit</th>
      <th>selftext</th>
      <th>author_fullname</th>
      <th>gilded</th>
      <th>title</th>
      <th>name_x</th>
      <th>subreddit_type</th>
      <th>ups</th>
      <th>total_awards_received</th>
      <th>is_reddit_media_domain</th>
      <th>...</th>
      <th>is_suspended</th>
      <th>banwave_year</th>
      <th>karma</th>
      <th>date_created</th>
      <th>year_graph_y</th>
      <th>year_y</th>
      <th>month_nr_y</th>
      <th>month_y</th>
      <th>day_y</th>
      <th>day_of_week_y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Blackpeople</td>
      <td></td>
      <td>t2_wr8hk</td>
      <td>0</td>
      <td>The Story of 10 Young People Killed in a Day b...</td>
      <td>t3_55n8or</td>
      <td>public</td>
      <td>3</td>
      <td>0</td>
      <td>False</td>
      <td>...</td>
      <td>NaN</td>
      <td>2018-May-09</td>
      <td>17.0</td>
      <td>2016-03-30 13:23:08</td>
      <td>2016.0</td>
      <td>2016</td>
      <td>2016-03</td>
      <td>March</td>
      <td>2016-03-30</td>
      <td>Wednesday</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Blackpeople</td>
      <td></td>
      <td>t2_wr8hk</td>
      <td>0</td>
      <td>Ramsey Orta: No regrets for shooting viral vid...</td>
      <td>t3_55celf</td>
      <td>public</td>
      <td>2</td>
      <td>0</td>
      <td>False</td>
      <td>...</td>
      <td>NaN</td>
      <td>2018-May-09</td>
      <td>17.0</td>
      <td>2016-03-30 13:23:08</td>
      <td>2016.0</td>
      <td>2016</td>
      <td>2016-03</td>
      <td>March</td>
      <td>2016-03-30</td>
      <td>Wednesday</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Blackpeople</td>
      <td></td>
      <td>t2_wr8hk</td>
      <td>0</td>
      <td>Police Brutality Facts: The Case Of Terrence S...</td>
      <td>t3_552gqq</td>
      <td>public</td>
      <td>2</td>
      <td>0</td>
      <td>False</td>
      <td>...</td>
      <td>NaN</td>
      <td>2018-May-09</td>
      <td>17.0</td>
      <td>2016-03-30 13:23:08</td>
      <td>2016.0</td>
      <td>2016</td>
      <td>2016-03</td>
      <td>March</td>
      <td>2016-03-30</td>
      <td>Wednesday</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Blackfellas</td>
      <td></td>
      <td>t2_wr8hk</td>
      <td>0</td>
      <td>Gordon's emotional HR helps top Mets as Marlin...</td>
      <td>t3_54q6v4</td>
      <td>public</td>
      <td>6</td>
      <td>0</td>
      <td>False</td>
      <td>...</td>
      <td>NaN</td>
      <td>2018-May-09</td>
      <td>17.0</td>
      <td>2016-03-30 13:23:08</td>
      <td>2016.0</td>
      <td>2016</td>
      <td>2016-03</td>
      <td>March</td>
      <td>2016-03-30</td>
      <td>Wednesday</td>
    </tr>
    <tr>
      <th>4</th>
      <td>blackculture</td>
      <td></td>
      <td>t2_wr8hk</td>
      <td>0</td>
      <td>Corporations Boycotted North Carolina over the...</td>
      <td>t3_54qb41</td>
      <td>public</td>
      <td>1</td>
      <td>0</td>
      <td>False</td>
      <td>...</td>
      <td>NaN</td>
      <td>2018-May-09</td>
      <td>17.0</td>
      <td>2016-03-30 13:23:08</td>
      <td>2016.0</td>
      <td>2016</td>
      <td>2016-03</td>
      <td>March</td>
      <td>2016-03-30</td>
      <td>Wednesday</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 71 columns</p>
</div>




```python
submitted_banwave_2019.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>subreddit</th>
      <th>selftext</th>
      <th>author_fullname</th>
      <th>gilded</th>
      <th>title</th>
      <th>name_x</th>
      <th>subreddit_type</th>
      <th>ups</th>
      <th>total_awards_received</th>
      <th>is_reddit_media_domain</th>
      <th>...</th>
      <th>is_suspended</th>
      <th>banwave_year</th>
      <th>karma</th>
      <th>date_created</th>
      <th>year_graph_y</th>
      <th>year_y</th>
      <th>month_nr_y</th>
      <th>month_y</th>
      <th>day_y</th>
      <th>day_of_week_y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>The_Europe</td>
      <td>https://s27.postimg.org/epmy7w943/1mac.jpg  \n...</td>
      <td>t2_178mh7</td>
      <td>0</td>
      <td>Macron moves in the Élysée Palace on shoulders...</td>
      <td>t3_66gpgy</td>
      <td>public</td>
      <td>10</td>
      <td>0</td>
      <td>False</td>
      <td>...</td>
      <td>NaN</td>
      <td>2019-Dec-06</td>
      <td>48.0</td>
      <td>2017-04-20 08:56:32</td>
      <td>2017.0</td>
      <td>2017</td>
      <td>2017-04</td>
      <td>April</td>
      <td>2017-04-20</td>
      <td>Thursday</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Eurosceptics</td>
      <td>[removed]</td>
      <td>t2_178mh7</td>
      <td>0</td>
      <td>Macron moves in the Élysée Palace on shoulders...</td>
      <td>t3_66grj3</td>
      <td>public</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>...</td>
      <td>NaN</td>
      <td>2019-Dec-06</td>
      <td>48.0</td>
      <td>2017-04-20 08:56:32</td>
      <td>2017.0</td>
      <td>2017</td>
      <td>2017-04</td>
      <td>April</td>
      <td>2017-04-20</td>
      <td>Thursday</td>
    </tr>
    <tr>
      <th>2</th>
      <td>europe</td>
      <td>[removed]</td>
      <td>t2_178mh7</td>
      <td>0</td>
      <td>Macron moves in the Élysée Palace on shoulders...</td>
      <td>t3_66gjdo</td>
      <td>public</td>
      <td>1</td>
      <td>0</td>
      <td>False</td>
      <td>...</td>
      <td>NaN</td>
      <td>2019-Dec-06</td>
      <td>48.0</td>
      <td>2017-04-20 08:56:32</td>
      <td>2017.0</td>
      <td>2017</td>
      <td>2017-04</td>
      <td>April</td>
      <td>2017-04-20</td>
      <td>Thursday</td>
    </tr>
    <tr>
      <th>3</th>
      <td>eupolitics</td>
      <td>https://s27.postimg.org/epmy7w943/1mac.jpg  \n...</td>
      <td>t2_178mh7</td>
      <td>0</td>
      <td>Macron moves in the Élysée Palace on shoulders...</td>
      <td>t3_66gnnb</td>
      <td>public</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>...</td>
      <td>NaN</td>
      <td>2019-Dec-06</td>
      <td>48.0</td>
      <td>2017-04-20 08:56:32</td>
      <td>2017.0</td>
      <td>2017</td>
      <td>2017-04</td>
      <td>April</td>
      <td>2017-04-20</td>
      <td>Thursday</td>
    </tr>
    <tr>
      <th>4</th>
      <td>eu</td>
      <td>[removed]</td>
      <td>t2_178mh7</td>
      <td>0</td>
      <td>Macron moves in the Élysée Palace on shoulders...</td>
      <td>t3_66glmq</td>
      <td>public</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>...</td>
      <td>NaN</td>
      <td>2019-Dec-06</td>
      <td>48.0</td>
      <td>2017-04-20 08:56:32</td>
      <td>2017.0</td>
      <td>2017</td>
      <td>2017-04</td>
      <td>April</td>
      <td>2017-04-20</td>
      <td>Thursday</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 71 columns</p>
</div>



# Analysis


```python
# no downvotes on any submission
same_values_submitted["downs"].value_counts()
```




    0    2909
    Name: downs, dtype: int64




```python
# no downvotes in comments either
same_values_comments["downs"].value_counts()
```




    0    1736
    Name: downs, dtype: int64



## Users banned in 2018 

### User creation over the years, users banned in 2018


```python
# Create the figure and the axes
fig, ax = plt.subplots()
# Set limits and labels
ax.set(title='Date of creation')

banned_2018 = about_banwave_2018.groupby(["year", "month"]).count().sort_values(by="year")["id"].plot(kind="line")
plt.xticks(rotation=25)
```




    (array([-5.,  0.,  5., 10., 15., 20., 25., 30., 35., 40.]),
     <a list of 10 Text xticklabel objects>)




![png](project-reddit-analysis_files/project-reddit-analysis_37_1.png)



```python
# banned_2018_line = about_banwave_2018.groupby(["year_graph", "month"], as_index=False).count().sort_values(by="year_graph")

# fig = px.line(banned_2018_line, x="year_graph", y="id", title="Account creation dates")
# fig.show()
```

### Subreddits most used by users banned in 2018


```python
# top_subreddits_2018.sort_values().plot(kind="barh")
# plt.xticks(rotation=90)

top_subreddits_2018 = submitted_banwave_2018.groupby("subreddit", as_index=False).count().sort_values(by="score", ascending=False).head()
fig = px.bar(top_subreddits_2018.sort_values(by="score", ascending=True), y="subreddit", x="score",  orientation='h',
             hover_data=["subreddit"], color="score",
             labels={'Score':'Nr of posts'}, height=400
            )
fig.show(renderer="png")
```


![png](project-reddit-analysis_files/project-reddit-analysis_40_0.png)



```python
#top_subreddits_2018.sort_values().plot(kind="barh")
#plt.xticks(rotation=90)
#import numpy as np
#df = pd.DataFrame(submitted_banwave_2018.groupby("subreddit", as_index=False).count().sort_values(by="score", ascending=False).head())
#cols_to_keep = ["subreddit", "score"]
#df[cols_to_keep].sort_values(by="score", ascending=True).plot.barh(x="subreddit", color="blue")
#range(min(df[cols_to_keep]["score"]),max(df[cols_to_keep]["score"]))
```

## Users banned in 2019

### User creation over the years, users banned in 2019


```python
# user creation over the years, 2019 banwave group
# Create the figure and the axes
fig, ax = plt.subplots()
ax.set(title='Date of creation')
about_banwave_2019.groupby(["year", "month"]).count().sort_values(by="year")["id"].plot(kind="line")
plt.xticks(rotation=25)
```




    (array([-2.5,  0. ,  2.5,  5. ,  7.5, 10. , 12.5, 15. , 17.5, 20. ]),
     <a list of 10 Text xticklabel objects>)




![png](project-reddit-analysis_files/project-reddit-analysis_44_1.png)


### Subreddits most used by users banned in 2018


```python
#top_subreddits_2019.sort_values().plot(kind="barh")
top_subreddits_2019 = submitted_banwave_2019.groupby("subreddit", as_index=False).count().sort_values(by="score", ascending=False).head(7)
fig = px.bar(top_subreddits_2019.sort_values(by="score", ascending=True), y="subreddit", x="score",  orientation='h',
             hover_data=["subreddit"], color="score",
             labels={'Score':'Nr of posts'}, height=400
            )
fig.show(renderer="png")
```


![png](project-reddit-analysis_files/project-reddit-analysis_46_0.png)


### Yearly trends

### Posts


```python
#  Posts 
yearly_trend_submitted = submitted[["score", "author", "subreddit", "title","date_posted"]].sort_values(by="score", ascending=False)
yearly_trend_submitted.groupby(pd.Grouper(key='date_posted', freq='M')).size().plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8b1291aad0>




![png](project-reddit-analysis_files/project-reddit-analysis_49_1.png)


### Spikes in post, by months


```python
#  Posts 
yearly_trend_submitted = submitted[["score", "author", "subreddit", "title","date_posted"]]
monthly_trend = yearly_trend_submitted.groupby(pd.Grouper(key='date_posted', freq='M'), as_index=False).size().sort_index()
monthly_trend[monthly_trend > 20].plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8b1006b910>




![png](project-reddit-analysis_files/project-reddit-analysis_51_1.png)


### Comments


```python
#  Comments 
yearly_trend_comments = comments[["score", "author", "subreddit","link_title", "body","date_posted"]].sort_values(by="score", ascending=False)
yearly_trend_comments.groupby(pd.Grouper(key='date_posted', freq='M')).size().plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8b12958250>




![png](project-reddit-analysis_files/project-reddit-analysis_53_1.png)


### Subreddits with most upvoted comments


```python
# top comments
top_comments = comments.groupby("subreddit", as_index=False)["score"].sum().sort_values(by="score", ascending=False).head(5)

fig = px.bar(top_comments.sort_values(by="score", ascending=True), y="subreddit", x="score",  orientation='h',
             hover_data=["subreddit"], color="score",
             labels={"score": "score"}, height=400
            )
fig.show(renderer="png")
```


![png](project-reddit-analysis_files/project-reddit-analysis_55_0.png)


### Year 2016 - US Election

### Number of posts in 2016 - monthy


```python
#  Posts for the year 2016 - peak in september 2016
top_2016_submitted = submitted[submitted["year"] == "2016"].sort_values(by="score", ascending=False)
top_2016_submitted.groupby(pd.Grouper(key='date_posted', freq='M')).size().plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8b143c4790>




![png](project-reddit-analysis_files/project-reddit-analysis_58_1.png)



```python
#  Posts for the year 2016 - peak in october 2016
top_2016_submitted = submitted[submitted["year"] == "2016"].sort_values(by="score", ascending=False)
top_2016_submitted.groupby(pd.Grouper(key='date_posted', freq='W')).size().plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8b12a2a9d0>




![png](project-reddit-analysis_files/project-reddit-analysis_59_1.png)



```python
#  Posts for the year 2016 - peak in october 2016
top_late_2016 = submitted[submitted["date_posted"].between("2016-09-01", "2016-12-31")][["date_posted","title","day"]]
top_late_2016.groupby(pd.Grouper(key='date_posted', freq='D')).size().plot()
#top_late_2016["day"].value_counts().sort_values()
#top_late_2016.groupby(pd.Grouper(key='date_posted', freq='D')).size().plot()
#fig = px.line(late_2016_line, x="value", y=top_late_2016.index, title='Line')
#fig.show()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8b146cee90>




![png](project-reddit-analysis_files/project-reddit-analysis_60_1.png)


### Subreddits where the users were most active, 2016


```python
top_subreddits_2016_bar1 = submitted[submitted["year"] == "2016"].groupby("subreddit", as_index=False).count().sort_values(by="score", ascending=False).head(10)
fig = px.bar(top_subreddits_2016_bar1.sort_values(by="num_comments"), y="subreddit", x="id",  orientation='h',
             hover_data=["subreddit"], color="score",
             labels={'id':'Nr of posts'}, height=400
            )
fig.show(renderer="png")
```


![png](project-reddit-analysis_files/project-reddit-analysis_62_0.png)


### Subreddit with posts generating the most comments, 2016


```python
top_subreddits_2016_bar2 = submitted[submitted["year"] == "2016"].groupby("subreddit", as_index=False).sum().sort_values(by="num_comments", ascending=False).head(10)
fig = px.bar(top_subreddits_2016_bar2.sort_values(by="num_comments"), y="subreddit", x="num_comments",  orientation='h',
             hover_data=["subreddit"], color="num_comments",
             labels={'num_comments':'Nr of comments'}, height=400
            )
fig.show(renderer="png")
```


![png](project-reddit-analysis_files/project-reddit-analysis_64_0.png)


###  Top comments in 2016


```python
#  Comments for the year 2016 - 
comments_2016 = comments[comments["year"] == "2016"].sort_values(by="id", ascending=False)
comments_2016.groupby(pd.Grouper(key='date_posted', freq='M')).size().plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8b14922290>




![png](project-reddit-analysis_files/project-reddit-analysis_66_1.png)


### Subreddits with most upvoted comments, 2016


```python
top_comments_2016_bar = comments[comments["year"] == "2016"].groupby("subreddit", as_index=False).sum().sort_values(by="score", ascending=False).head(10)
fig = px.bar(top_comments_2016_bar.sort_values(by="score"), y="subreddit", x="score",  orientation='h',
             hover_data=["subreddit"], color="score",
             labels={'Score':'Nr of posts'}, height=400
            )
fig.show(renderer="png")
```


![png](project-reddit-analysis_files/project-reddit-analysis_68_0.png)


### Year 2019 


```python
#  Comments for the year 2019 - 
top_2019_comments = comments[comments["year"] == "2019"].sort_values(by="score", ascending=False)
top_2019_comments.groupby(pd.Grouper(key='date_posted', freq='M')).size().plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8b1a0f9490>




![png](project-reddit-analysis_files/project-reddit-analysis_70_1.png)



```python
#  Posts for the year 2019 - 
top_2019_submitted = submitted[submitted["year"] == "2019"][["score", "author", "subreddit", "title","date_posted"]].sort_values(by="score", ascending=False)
top_2019_submitted.groupby(pd.Grouper(key='date_posted', freq='M')).size().plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8b19d1c4d0>




![png](project-reddit-analysis_files/project-reddit-analysis_71_1.png)


### Subreddits generating the most comments, 2019


```python
top_subreddits_2019_bar = submitted[submitted["year"] == "2019"].groupby("subreddit", as_index=False).sum().sort_values(by="num_comments", ascending=False).head(10)
fig = px.bar(top_subreddits_2019_bar.sort_values(by="num_comments"), y="subreddit", x="num_comments",  orientation='h',
             hover_data=["subreddit"], color="num_comments",
             labels={"num_comments":'Nr of posts'}, height=400
            )
fig.show(renderer="png")
```


![png](project-reddit-analysis_files/project-reddit-analysis_73_0.png)


# Analyzing content


```python
from nltk.tokenize import RegexpTokenizer 
from nltk.stem.porter import PorterStemmer
```


```python
custom_list=["jj", "oo"] # filter some additional words
def stem_text(text):    
    stemmer = PorterStemmer()
    tokenizer = RegexpTokenizer(r"\w+")
    text_token = tokenizer.tokenize(text.lower())
    return " ".join([stemmer.stem(x) for x in text_token if x not in stopwords.words(["english", "spanish"]) and x not in custom_list])

def lem_text(text): # "lemmarize" text to get real words from all their variations
    tokenizer = RegexpTokenizer(r"\w+")
    text_token = tokenizer.tokenize(text.lower())
    lmtzr = WordNetLemmatizer()
    return " ".join( [lmtzr.lemmatize(x) for x in text_token if x not in stopwords.words(["english", "spanish"])and x not in custom_list] )

def filter_text(text):    
    tokenizer = RegexpTokenizer(r"\w+")
    text_token = tokenizer.tokenize(text.lower())  
    return " ".join([x for x in text_token if x not in stopwords.words(["english", "spanish"])and x not in custom_list])

```


```python
# different filters for the titles
submitted_banwave_2018_stemmed_text = submitted_banwave_2018["title"].apply(stem_text)
submitted_banwave_2019_lemmarized_text = submitted_banwave_2019["title"].apply(lem_text)
# for comments
comments_lemmarized_text = comments["body"].apply(lem_text)
```

### Posts by users banned in 2018


```python
# Wordcloud of submission titles users banned in 2018
submitted_2018_wordcloud = WordCloud(max_words=15, background_color="white").generate(' '.join(submitted_banwave_2018_stemmed_text))

plt.imshow(submitted_2018_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_79_0.png)


### Posts by users banned in 2019


```python
# Wordcloud of submitssion titles users banned in 2019
submitted_2019_wordcloud = WordCloud(collocations=True, max_words=15, background_color="white").generate(' '.join(submitted_banwave_2019_lemmarized_text))
plt.imshow(submitted_2019_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_81_0.png)


### Comments by all users


```python
# wordcloud of all comments
comments_wordcloud = WordCloud(max_words=15, background_color="white").generate(" ".join(comments_lemmarized_text))
plt.imshow(comments_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_83_0.png)


# Sentiment Analysis


```python

# https://github.com/dend/data-explorations/blob/master/notebooks/sentiment-analysis-reddit.ipynb
sia = SentimentIntensityAnalyzer()
def analyze_content(title):
    text_list = []
    for line in title:
        pol_score = sia.polarity_scores(line)
        pol_score['text'] = line
        text_list.append(pol_score)
    result = pd.DataFrame.from_records(text_list)
    result["sentiment"] = 0
    result.loc[result["compound"] > 0.1, "sentiment"] = "positive"
    result.loc[result["compound"] < -0.1, "sentiment"] = "negative"
    result.loc[result["compound"].between(-0.1, 0.1) , "sentiment"] = "neutral"
    return result
```


```python
sentiment_analysis_submitted = analyze_content(submitted["title"])
sentiment_analysis_comments = analyze_content(comments["body"])
sentiment_analysis_submitted_banwave_2019 = analyze_content(submitted_banwave_2019["title"])
```

## Sentiment analysis of posts


```python
group = sentiment_analysis_submitted.groupby("sentiment", as_index=False).count()
colors = ["lightcoral", "lightblue", "lightgreen"]
fig = go.Figure(data=[go.Bar(x=group["sentiment"], y=group["pos"], orientation='v',
                    marker_color=colors,)])

fig.update_layout(title_text="Sentiment analysis of posts")
fig.show(renderer="png")
```


![png](project-reddit-analysis_files/project-reddit-analysis_88_0.png)


### Sentiment analysis of comments


```python
group = sentiment_analysis_comments.groupby("sentiment", as_index=False).count()
fig = go.Figure(data=[go.Bar(x=group["sentiment"], y=group["pos"], orientation='v',
                    marker_color=colors,)])

fig.update_layout(title_text="Sentiment analysis of comments")
fig.show(renderer="png")
```


![png](project-reddit-analysis_files/project-reddit-analysis_90_0.png)


### Words used in positive and negative posts and comments 


```python
# function that generates lists of word in negative, neutral and positive sentiments. Then generate wordcloud
def classify_sentiment(table):
    list_neg, list_neu, list_pos = [], [], []
    if "text" in table.columns and "sentiment" in table.columns:
        for i in range(len(table)):
            row = table.loc[i] 
            if "negative" in row["sentiment"]:
                list_neg.append(row["text"])
            elif "positive" in row["sentiment"]:
                list_pos.append(row["text"])
            else:
                list_neu.append(row["text"])
        result = pd.DataFrame({"negative":pd.Series(list_neg),"neutral":pd.Series(list_neu), "positive":pd.Series(list_pos) })
        return result.fillna("")
                
```


```python
# categorizes posts and comments according to sentiment analysis
classified_submitted = classify_sentiment(sentiment_analysis_submitted)
classified_comments = classify_sentiment(sentiment_analysis_comments)
classified_submited_banwave_2019 = classify_sentiment(sentiment_analysis_submitted_banwave_2019)
# generates lists of words for wordclouds
classified_submitted_neg = classified_submitted["negative"].apply(lem_text)
classified_submitted_pos = classified_submitted["positive"].apply(lem_text)
classified_submitted_neu = classified_submitted["neutral"].apply(lem_text)
classified_submited_banwave_2019_pos = classified_submited_banwave_2019["positive"].apply(lem_text)
classified_submited_banwave_2019_neg = classified_submited_banwave_2019["negative"].apply(lem_text)
classified_submited_banwave_2019_neu = classified_submited_banwave_2019["neutral"].apply(lem_text)


classified_comments_neg = classified_comments["negative"].apply(lem_text)
classified_comments_pos = classified_comments["positive"].apply(lem_text)
classified_comments_neu = classified_comments["neutral"].apply(lem_text)
```

### Wordcloud of all negative posts


```python
# wordcloud of all negative posts
submitted_neg_wordcloud = WordCloud(max_words=15, background_color="white").generate(" ".join(classified_submitted_neg))
plt.imshow(submitted_neg_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_95_0.png)


### Wordcloud of all positive posts


```python
# wordcloud of all positive posts
submitted_pos_wordcloud = WordCloud(max_words=15, background_color="white").generate(" ".join(classified_submitted_pos))
plt.imshow(submitted_pos_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_97_0.png)


### Wordcloud of all negative posts - banned in 2019


```python
# wordcloud of all negative posts - banned in 2019
submitted_pos_wordcloud_banwave_2019_neg = WordCloud(max_words=15, background_color="white").generate(" ".join(classified_submited_banwave_2019_neg))
plt.imshow(submitted_pos_wordcloud_banwave_2019_neg, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_99_0.png)


### Wordcloud of all positive posts - banned in 2019


```python
# wordcloud of all positive posts - banned in 2019
submitted_pos_wordcloud_banwave_2019_pos = WordCloud(max_words=15, background_color="white").generate(" ".join(classified_submited_banwave_2019_pos))
plt.imshow(submitted_pos_wordcloud_banwave_2019_pos, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_101_0.png)


### Wordcloud of all negative comments


```python
# wordcloud of all negative comments
comments_neg_wordcloud = WordCloud(max_words=15, background_color="white").generate(" ".join(classified_comments_neg))
plt.imshow(comments_neg_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_103_0.png)


### Wordcloud of all positive comments


```python
# wordcloud of all positive comments
comments_pos_wordcloud = WordCloud(max_words=15, background_color="white").generate(" ".join(classified_comments_pos))
plt.imshow(comments_pos_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_105_0.png)


### Wordcloud of all neutral posts


```python
# wordcloud of all neutral posts
submitted_neu_wordcloud = WordCloud(max_words=15, background_color="white").generate(" ".join(classified_submitted_neu))
plt.imshow(submitted_neu_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_107_0.png)


### Wordcloud of all neutral comments


```python
# wordcloud of all neutral comments
comments_neu_wordcloud = WordCloud(max_words=15, background_color="white").generate(" ".join(classified_comments_neu))
plt.imshow(comments_neu_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_109_0.png)


### Wordcloud of all neutral posts - banned in 2019


```python
# wordcloud of all neutral posts - banned in 2019
submitted_pos_wordcloud_banwave_2019_neu = WordCloud(max_words=15, background_color="white").generate(" ".join(classified_submited_banwave_2019_neu))
plt.imshow(submitted_pos_wordcloud_banwave_2019_neu, interpolation="bilinear")
plt.axis("off")
plt.show()
```


![png](project-reddit-analysis_files/project-reddit-analysis_111_0.png)


### Unused


```python
# creating groups 
top_submit_2018 = submitted_banwave_2018[submitted_banwave_2018["ups"] > 1000]
top_submit_2019 = submitted_banwave_2019[submitted_banwave_2019["ups"] > 10]
top_comments_2019 = comments_banwave_2019[comments_banwave_2019["ups"] > 10]
top_comments_2018 = comments_banwave_2018[comments_banwave_2018["ups"] > 50]
top_users_2018 = about_banwave_2018[about_banwave_2018["karma"] > 10000]
top_users_2019 = about_banwave_2018[about_banwave_2018["karma"] > 10]
```


```python
# top users  banned users in 2019
#about_banwave_2019[["karma","name", "date_created"]].sort_values(by="karma",ascending=False).head()
```


```python
# top posts for users banned in  2019
#submitted_banwave_2019[["ups","author","subreddit","title","score","date_posted","gilded"]].sort_values(by="ups", ascending=False).head()
```


```python
# comments with the most upvotes for users banned in  2019
#comments_banwave_2019[["ups","author","subreddit","link_title","body","score","date_posted","gilded"]].sort_values(by="score", ascending=False).head()
```


```python
# How many comments with a controvsersiality value of 1 ?
#comments.groupby("controversiality").count()
```


```python
#comments[comments["controversiality"]> 0][["controversiality","ups","author","subreddit","link_title","body","score","gilded"]].sort_values(by="ups", ascending=False).head()
```


```python
# posts by the top 5 users banned in 2018
top_1_post_2018 = submitted_banwave_2018[submitted_banwave_2018["author"] == "rubinjer"][["score","author","subreddit","title","selftext", "date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_2_post_2018 = submitted_banwave_2018[submitted_banwave_2018["author"] == "shomyo"][["score","author","subreddit","title","selftext", "date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_3_post_2018 = submitted_banwave_2018[submitted_banwave_2018["author"] == "Kevin_Milner"][["score","author","subreddit","title","selftext", "date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_4_post_2018 = submitted_banwave_2018[submitted_banwave_2018["author"] == "WhatImDoindHere"][["score","author","subreddit","title","selftext","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_5_post_2018 = submitted_banwave_2018[submitted_banwave_2018["author"] == "BerskyN"][["score","author","subreddit","title","selftext","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top5_2018 = pd.concat([top_1_post_2018, top_2_post_2018, top_3_post_2018, top_4_post_2018, top_5_post_2018])
```


```python
# posts by the top 5 users banned in 2019
top_1_post_2019 = submitted_banwave_2019[submitted_banwave_2019["author"] == "alabelm"][["score","author","subreddit","title","selftext","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_2_post_2019 = submitted_banwave_2019[submitted_banwave_2019["author"] == "gregoratior"][["score","author","subreddit","title","selftext","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_3_post_2019 = submitted_banwave_2019[submitted_banwave_2019["author"] == "krakodoc"][["score","author","subreddit","title","selftext","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_4_post_2019 = submitted_banwave_2019[submitted_banwave_2019["author"] == "rabbier"][["score","author","subreddit","title","selftext","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_5_post_2019 = submitted_banwave_2019[submitted_banwave_2019["author"] == "KlausSteiner"][["score","author","subreddit","title","selftext","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top5_2019 = pd.concat([top_1_post_2019, top_2_post_2019, top_3_post_2019, top_4_post_2019, top_5_post_2019])
```


```python
## Users banned in 2018 posted in various subreddit
#top5_2018.sort_values(by="score", ascending=False).head()
```


```python
## Users banned in 2019 posted in political subreddits, mostly concerning Europe
#top5_2019.sort_values(by="score", ascending=False).head()
```


```python
# comments by the top 5 users banned in 2018
top_1_comments_2018 = comments_banwave_2018[comments_banwave_2018["author"] == "rubinjer"][["score","author","subreddit","body","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_2_comments_2018 = comments_banwave_2018[comments_banwave_2018["author"] == "shomyo"][["score","author","subreddit","body","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_3_comments_2018 = comments_banwave_2018[comments_banwave_2018["author"] == "Kevin_Milner"][["score","author","subreddit","body","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_4_comments_2018 = comments_banwave_2018[comments_banwave_2018["author"] == "WhatImDoindHere"][["score","author","subreddit","body","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_5_comments_2018 = comments_banwave_2018[comments_banwave_2018["author"] == "BerskyN"][["score","author","subreddit","body","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top5_comments_2018 = pd.concat([top_1_comments_2018, top_2_comments_2018, top_3_comments_2018, top_4_comments_2018, top_5_comments_2018])

# comments by the top 5 users banned in 2019
top_1_comments_2019 = comments_banwave_2019[comments_banwave_2019["author"] == "alabelm"][["score","author","subreddit","body","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_2_comments_2019 = comments_banwave_2019[comments_banwave_2019["author"] == "gregoratior"][["score","author","subreddit","body","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_3_comments_2019 = comments_banwave_2019[comments_banwave_2019["author"] == "krakodoc"][["score","author","subreddit","body","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_4_comments_2019 = comments_banwave_2019[comments_banwave_2019["author"] == "rabbier"][["score","author","subreddit","body","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_5_comments_2019 = comments_banwave_2019[comments_banwave_2019["author"] == "KlausSteiner"][["score","author","subreddit","body","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top5_comments_2019 = pd.concat([top_1_comments_2019, top_2_comments_2019, top_3_comments_2019, top_4_comments_2019, top_5_comments_2019])
```


```python
## users banned in 2018 commented in various topics, some "fringe" like cryptocurrency
#top5_comments_2018.sort_values(by="score", ascending=False).head()
```


```python
## users banned in 2019 barely commented on topics, only one of the top 5 did
#top5_comments_2019.sort_values(by="score", ascending=False)
```


```python
# top users first banned users
#about_banwave_2018[["karma","name", "date_created"]].sort_values(by="karma",ascending=False).head()
```


```python
# top posts for first banned users
#submitted_banwave_2018[["ups","author","subreddit","title","score","date_posted","gilded"]].sort_values(by="ups", ascending=False).head(10)
```


```python
# comments with the most upvotes for first banned users
#comments_banwave_2018[["controversiality","ups","author","subreddit","link_title","body","score","date_posted","gilded"]].sort_values(by="ups", ascending=False).head()
```


```python
# posts by the top 5 users banned in 2018
top_1_post_2018 = submitted_banwave_2018[submitted_banwave_2018["author"] == "rubinjer"][["score","author","subreddit","title","selftext", "date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_2_post_2018 = submitted_banwave_2018[submitted_banwave_2018["author"] == "shomyo"][["score","author","subreddit","title","selftext", "date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_3_post_2018 = submitted_banwave_2018[submitted_banwave_2018["author"] == "Kevin_Milner"][["score","author","subreddit","title","selftext", "date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_4_post_2018 = submitted_banwave_2018[submitted_banwave_2018["author"] == "WhatImDoindHere"][["score","author","subreddit","title","selftext","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top_5_post_2018 = submitted_banwave_2018[submitted_banwave_2018["author"] == "BerskyN"][["score","author","subreddit","title","selftext","date_posted","gilded"]].sort_values(by="score", ascending=False).head(5)
top5_2018 = pd.concat([top_1_post_2018, top_2_post_2018, top_3_post_2018, top_4_post_2018, top_5_post_2018])
```
